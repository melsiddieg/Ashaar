{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24720c4d-6ec2-484c-951d-f07273c43e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 00:54:50.171172: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-25 00:54:50.231880: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-25 00:54:50.233716: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-25 00:54:51.272504: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting the pretrained models ... \n",
      "File exists: deep-learning-models.zip\n",
      "load diacritization model ... \n",
      "WARNING: could not check git hash. 'git_hash'\n",
      "loading from /teamspace/studios/this_studio/Ashaar/deep-learning-models/log_dir_ashaar/ashaar_proc.base.cbhg/models/10000-snapshot.pt\n",
      "load meter classification model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 00:55:02.986959: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-06-25 00:55:02.990858: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n\n    TypeError: outer_factory.<locals>.inner_factory.<locals>.tf__call() missing 1 required positional argument: 'training'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# ─── 1.  (Re)build a fresh analyser instance ───────────────────────────────\u001b[39;00m\n\u001b[1;32m     11\u001b[0m root      \u001b[38;5;241m=\u001b[39m Path\u001b[38;5;241m.\u001b[39mcwd()                        \u001b[38;5;66;03m# folder that holds poetry_diacritizer/\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m analysis  \u001b[38;5;241m=\u001b[39m \u001b[43mBaitAnalysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabs_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# use_cbhg=True by default\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Restore the meter weights\u001b[39;00m\n\u001b[1;32m     15\u001b[0m ckpt \u001b[38;5;241m=\u001b[39m root \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeep-learning-models/meters_model/cp.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Ashaar/Ashaar/bait_analysis.py:73\u001b[0m, in \u001b[0;36mBaitAnalysis.__init__\u001b[0;34m(self, abs_path, use_cbhg)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_encoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiac_model\u001b[38;5;241m.\u001b[39mtext_encoder\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload meter classification model ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMETERS_MODEL \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_transformer_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMETERS_MODEL\u001b[38;5;241m.\u001b[39mload_weights(\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mabs_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/deep-learning-models/meters_model/cp.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload embedding model ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Ashaar/Ashaar/models.py:99\u001b[0m, in \u001b[0;36mcreate_transformer_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m x \u001b[38;5;241m=\u001b[39m embedding_layer(inputs)\n\u001b[1;32m     98\u001b[0m transformer_block \u001b[38;5;241m=\u001b[39m TransformerBlock(embed_dim, num_heads, ff_dim)\n\u001b[0;32m---> 99\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mFlatten()(x)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# x = layers.Dropout(0.1)(x)\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/engine/base_layer_v1.py:838\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m    836\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[1;32m    837\u001b[0m     ):\n\u001b[0;32m--> 838\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m tf\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mOperatorNotAllowedInGraphError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    841\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are attempting to use Python control \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflow in a layer that was not declared to be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    848\u001b[0m     )\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:692\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 692\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m    693\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:689\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconverted_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meffective_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[38;5;241m=\u001b[39m converted_f(\u001b[38;5;241m*\u001b[39meffective_args)\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n\n    TypeError: outer_factory.<locals>.inner_factory.<locals>.tf__call() missing 1 required positional argument: 'training'\n"
     ]
    }
   ],
   "source": [
    "# ─── 0.  Setup ─────────────────────────────────────────────────────────────\n",
    "from pathlib import Path\n",
    "from Ashaar.bait_analysis import BaitAnalysis\n",
    "import tensorflow as tf, functools, torch, numpy as np\n",
    "\n",
    "# Disable eager for TF-1.x code inside Ashaar\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "torch.load = functools.partial(torch.load, weights_only=False)\n",
    "\n",
    "# ─── 1.  (Re)build a fresh analyser instance ───────────────────────────────\n",
    "root      = Path.cwd()                        # folder that holds poetry_diacritizer/\n",
    "analysis  = BaitAnalysis(abs_path=str(root))  # use_cbhg=True by default\n",
    "\n",
    "# Restore the meter weights\n",
    "ckpt = root / \"deep-learning-models/meters_model/cp.ckpt\"\n",
    "analysis.METERS_MODEL.load_weights(str(ckpt)).expect_partial()\n",
    "\n",
    "# ─── 2.  Patch ONLY the `clean()` method (safe, non-recursive) ─────────────\n",
    "import regex as re\n",
    "from types import MethodType\n",
    "\n",
    "# Keep an *immutable* reference to the original cleaner\n",
    "_orig_clean = analysis.text_encoder.clean            # bound method\n",
    "\n",
    "# Pre-compiled regex that recognises Arabic ḥarakāt\n",
    "_ARABIC_DIAC = re.compile(r'[\\u064B-\\u065F]')        # Fathatan → Sukūn\n",
    "\n",
    "def smart_clean(self, txt: str) -> str:\n",
    "    \"\"\"\n",
    "    If the string already contains diacritics → return unchanged.\n",
    "    Otherwise → run the *original* normaliser exactly once.\n",
    "    \"\"\"\n",
    "    if _ARABIC_DIAC.search(txt):\n",
    "        return txt\n",
    "    return _orig_clean(txt)\n",
    "\n",
    "# Bind the wrapper back onto the *existing* encoder object\n",
    "analysis.text_encoder.clean = MethodType(smart_clean, analysis.text_encoder)\n",
    "\n",
    "# Optional: expose both versions for debugging / unit tests\n",
    "analysis.text_encoder.clean_raw   = _orig_clean\n",
    "analysis.text_encoder.clean_smart = analysis.text_encoder.clean\n",
    "\n",
    "# ─── 3.  Sanity check ──────────────────────────────────────────────────────\n",
    "bayt = \"ألا ليت شعري هل أبيتن ليلة # بجنب الغضى أزجي القلاص النواجيا\"\n",
    "result = analysis.analyze(bayt, override_tashkeel=False)\n",
    "\n",
    "print(\"\\n— ANALYSIS —\")\n",
    "for k, v in result.items():\n",
    "    if isinstance(v, list):\n",
    "        print(f\"{k:18}: {v[:3]}{' …' if len(v) > 3 else ''}\")   # shorten long lists\n",
    "    else:\n",
    "        print(f\"{k:18}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aea1c1-8c7e-47f6-af70-4ebbdff24183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
